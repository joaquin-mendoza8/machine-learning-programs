{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 8: Optimization of Deep Neural Networks\n",
    "\n",
    "**Total points: 15**\n",
    "\n",
    "In this assignment, you will implement a multiple layer feed-forward neural network for a multi-class classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.25.1)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 1: Build a deep neural network model\n",
    "**Points: 3**\n",
    "\n",
    "Implement the `NeuralNetModel1` class. The model takes a $28\\times 28$ grey-scale image as input, and pass it through a deep neural network.\n",
    "\n",
    "The network has 2 hidden layers and 1 output layers, whose sizes are: 512 -> 512 -> 10. That is, the number of output classes is 10. The activation function for each hidden layer is `ReLU`.\n",
    "\n",
    "The input image is first passed through a `nn.Flatten()` layer so that a 2D tensor becomes 1D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetModel1, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.flatten = nn.Flatten() # Use nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # Input size is 28*28\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 512), # 512 -> 512\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.Linear(512, 10), # 512 -> 10\n",
    "        )\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        x = self.flatten(x) # Call self.flatten()\n",
    "        logits = self.linear_relu_stack(x) # Call self.linear_relu_stack()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([5, 28, 28])\n",
      "output size: torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "sample_input = torch.randn(5, 28, 28)\n",
    "print('input size:', sample_input.size())\n",
    "\n",
    "model1 = NeuralNetModel1()\n",
    "with torch.no_grad():\n",
    "    output = model1(sample_input)\n",
    "print('output size:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "input size: torch.Size([5, 28, 28])\\\n",
    "output size: torch.Size([5, 10])\n",
    "\n",
    "---\n",
    "\n",
    "## Task 2: Use dataloader\n",
    "**Points: 1**\n",
    "\n",
    "Download the FashionMNIST dataset provided by PyTorch to the folder \"data\", which takes some time for the first time execution.\n",
    "Use the `DataLoader` module to wrap the loaded training and test data. Specify the `batch_size` correctly for both training and test dataloader.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, # True\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # False\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "### START YOUR CODE ###\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size) # Specify data source and batch size correctly\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 60000\n",
      "Testing data size: 10000\n",
      "X size: torch.Size([64, 1, 28, 28])\n",
      "y size: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "print('Training data size:', len(training_data))\n",
    "print('Testing data size:', len(test_data))\n",
    "\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    X, y = batch\n",
    "    print('X size:', X.size())\n",
    "    print('y size:', y.size())\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "Training data size: 60000\\\n",
    "Testing data size: 10000\\\n",
    "X size: torch.Size([64, 1, 28, 28])\\\n",
    "y size: torch.Size([64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3: Define loss and optimizer\n",
    "**Points: 1**\n",
    "\n",
    "Use `nn.CrossEntropyLoss()` as the loss function, and use `torch.optim.SGD()` as the optimizer. Specify the arguments for `SGD()`, including the learning rate correctly.\n",
    "\n",
    "See <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html> and <https://pytorch.org/docs/stable/optim.html> for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "### START YOUR CODE ###\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n",
      "<class 'torch.optim.sgd.SGD'>\n"
     ]
    }
   ],
   "source": [
    "# Do not change the test code here\n",
    "print(loss_fn)\n",
    "print(type(optimizer_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "CrossEntropyLoss()\n",
    "<class 'torch.optim.sgd.SGD'>\n",
    "\n",
    "---\n",
    "\n",
    "## Task 4: Implement train and test functions\n",
    "**Points: 6**\n",
    "\n",
    "Implement the code for training the model in `train()`. Implement the code for testing the model in `test()`. For the backpropagation step, you need to first zero out all gradients by calling `optimizer.zero_grad()` before carrying out `backward()` and `step()` to update parameters.\n",
    "\n",
    "In `test()`, you need to calculate the number of correct prediction in the current batch, and add it to the `correct` variable.\n",
    "Finally, you need to divide `correct` by the total number of test examples to obtain the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, verbose=True):\n",
    "    for i, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        ### START YOUR CODE ###\n",
    "        pred = model(X) # Get the prediction output from model\n",
    "        loss = loss_fn(pred, torch.tensor(y)) # compute loss by calling loss_fn()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        # Backpropagation\n",
    "        ### START YOUR CODE ###\n",
    "        optimizer.zero_grad() # zero_grad()\n",
    "        loss.backward() # backward()\n",
    "        optimizer.step() # step()\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        if verbose and i % 100 == 0:\n",
    "            loss = loss.item()\n",
    "            current_step = i * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current_step:>5d}/{len(dataloader.dataset):>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    for X, y in dataloader:\n",
    "        ### START YOUR CODE ###\n",
    "        pred = model(X) # Similar to how it is computed in train()\n",
    "        loss = loss_fn(pred, torch.tensor(y))\n",
    "        test_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).sum().item() # Add the number of correct prediction in the current batch to `correct`\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    ### START YOUR CODE ###\n",
    "    test_acc = correct / len(dataloader.dataset) # Use `correct` to compute accuracy\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, execute the following cell to start the training and testing loop. Make sure that the cell containing the loss function and optimizers has already been executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296701  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/3999771966.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y)) # compute loss by calling loss_fn()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.284246  [ 6400/60000]\n",
      "loss: 2.266165  [12800/60000]\n",
      "loss: 2.270359  [19200/60000]\n",
      "loss: 2.252517  [25600/60000]\n",
      "loss: 2.231633  [32000/60000]\n",
      "loss: 2.239763  [38400/60000]\n",
      "loss: 2.206202  [44800/60000]\n",
      "loss: 2.202750  [51200/60000]\n",
      "loss: 2.174662  [57600/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/2174331187.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 2.171465 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.178695  [    0/60000]\n",
      "loss: 2.166123  [ 6400/60000]\n",
      "loss: 2.112747  [12800/60000]\n",
      "loss: 2.130645  [19200/60000]\n",
      "loss: 2.084812  [25600/60000]\n",
      "loss: 2.037295  [32000/60000]\n",
      "loss: 2.054001  [38400/60000]\n",
      "loss: 1.979531  [44800/60000]\n",
      "loss: 1.982396  [51200/60000]\n",
      "loss: 1.910793  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Avg loss: 1.913633 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.944625  [    0/60000]\n",
      "loss: 1.911709  [ 6400/60000]\n",
      "loss: 1.797972  [12800/60000]\n",
      "loss: 1.834717  [19200/60000]\n",
      "loss: 1.737232  [25600/60000]\n",
      "loss: 1.692372  [32000/60000]\n",
      "loss: 1.701046  [38400/60000]\n",
      "loss: 1.604598  [44800/60000]\n",
      "loss: 1.627080  [51200/60000]\n",
      "loss: 1.516717  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.542021 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.613822  [    0/60000]\n",
      "loss: 1.572238  [ 6400/60000]\n",
      "loss: 1.423322  [12800/60000]\n",
      "loss: 1.487494  [19200/60000]\n",
      "loss: 1.381459  [25600/60000]\n",
      "loss: 1.379044  [32000/60000]\n",
      "loss: 1.382804  [38400/60000]\n",
      "loss: 1.307106  [44800/60000]\n",
      "loss: 1.343910  [51200/60000]\n",
      "loss: 1.239233  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 1.270229 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.355614  [    0/60000]\n",
      "loss: 1.330863  [ 6400/60000]\n",
      "loss: 1.165464  [12800/60000]\n",
      "loss: 1.263085  [19200/60000]\n",
      "loss: 1.144842  [25600/60000]\n",
      "loss: 1.173151  [32000/60000]\n",
      "loss: 1.188245  [38400/60000]\n",
      "loss: 1.121187  [44800/60000]\n",
      "loss: 1.165412  [51200/60000]\n",
      "loss: 1.079880  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 1.102932 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.182023  [    0/60000]\n",
      "loss: 1.179154  [ 6400/60000]\n",
      "loss: 0.998187  [12800/60000]\n",
      "loss: 1.125661  [19200/60000]\n",
      "loss: 1.001185  [25600/60000]\n",
      "loss: 1.036262  [32000/60000]\n",
      "loss: 1.070051  [38400/60000]\n",
      "loss: 1.002471  [44800/60000]\n",
      "loss: 1.048715  [51200/60000]\n",
      "loss: 0.981323  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.995857 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.061847  [    0/60000]\n",
      "loss: 1.080974  [ 6400/60000]\n",
      "loss: 0.883553  [12800/60000]\n",
      "loss: 1.035266  [19200/60000]\n",
      "loss: 0.912068  [25600/60000]\n",
      "loss: 0.940015  [32000/60000]\n",
      "loss: 0.993524  [38400/60000]\n",
      "loss: 0.923604  [44800/60000]\n",
      "loss: 0.967150  [51200/60000]\n",
      "loss: 0.914721  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 0.922173 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.972427  [    0/60000]\n",
      "loss: 1.012113  [ 6400/60000]\n",
      "loss: 0.799890  [12800/60000]\n",
      "loss: 0.970490  [19200/60000]\n",
      "loss: 0.853134  [25600/60000]\n",
      "loss: 0.868751  [32000/60000]\n",
      "loss: 0.939850  [38400/60000]\n",
      "loss: 0.869767  [44800/60000]\n",
      "loss: 0.907701  [51200/60000]\n",
      "loss: 0.866196  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 0.868536 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.902871  [    0/60000]\n",
      "loss: 0.960343  [ 6400/60000]\n",
      "loss: 0.736595  [12800/60000]\n",
      "loss: 0.921306  [19200/60000]\n",
      "loss: 0.811516  [25600/60000]\n",
      "loss: 0.814859  [32000/60000]\n",
      "loss: 0.899155  [38400/60000]\n",
      "loss: 0.831844  [44800/60000]\n",
      "loss: 0.863129  [51200/60000]\n",
      "loss: 0.828615  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.827641 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.847125  [    0/60000]\n",
      "loss: 0.918841  [ 6400/60000]\n",
      "loss: 0.687080  [12800/60000]\n",
      "loss: 0.882684  [19200/60000]\n",
      "loss: 0.780202  [25600/60000]\n",
      "loss: 0.773576  [32000/60000]\n",
      "loss: 0.866391  [38400/60000]\n",
      "loss: 0.804114  [44800/60000]\n",
      "loss: 0.828844  [51200/60000]\n",
      "loss: 0.798179  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 0.795203 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "### START YOUR CODE ###\n",
    "optimizer_sgd = torch.optim.SGD(model1.parameters(), lr=learning_rate) # Because the model1 is reset, the optimizer also needs redefined.\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_sgd, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "The test accuracy from the last epoch should be above 70%.\n",
    "\n",
    "---\n",
    "\n",
    "Next, train an ADAM optimizer. Note that the model needs be reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/3999771966.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y)) # compute loss by calling loss_fn()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/2174331187.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.299667  [    0/60000]\n",
      "loss: 2.304362  [ 6400/60000]\n",
      "loss: 2.308612  [12800/60000]\n",
      "loss: 2.306053  [19200/60000]\n",
      "loss: 2.307977  [25600/60000]\n",
      "loss: 2.295567  [32000/60000]\n",
      "loss: 2.299925  [38400/60000]\n",
      "loss: 2.299843  [44800/60000]\n",
      "loss: 2.299171  [51200/60000]\n",
      "loss: 2.294061  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.1%, Avg loss: 2.302712 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetModel1() # Reset the model\n",
    "\n",
    "### START YOUR CODE ###\n",
    "optimizer_adam = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model1, loss_fn, optimizer_sgd, verbose=True) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model1, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Expected output**:\n",
    "\n",
    "You can find that the training converges much faster using ADAM.\n",
    "\n",
    "---\n",
    "\n",
    "## Task 5: Add batchnorm and dropout\n",
    "**Points: 4**\n",
    "\n",
    "Use `torch.nn.BatchNorm1d()` and `nn.Dropout()` after the ReLU activation of each hidden layer. `Batchnorm1d()` takes the size of previous activation as input. `Dropout()` takes the probability of dropout as input.\n",
    "\n",
    "For more information, see <https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html> and <https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetModel2(nn.Module):\n",
    "    def __init__(self, dropout = 0.1): # Note the additional dropout parameter here\n",
    "        \"\"\"\n",
    "        :param dropout: float, the probability of dropout\n",
    "        \"\"\"\n",
    "        super(NeuralNetModel2, self).__init__()\n",
    "        ### START YOUR CODE ###\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(), # ReLU\n",
    "            nn.BatchNorm1d(512), # Batchnorm\n",
    "            nn.Dropout(dropout), # Dropout, use the `dropout` parameter\n",
    "\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### START YOUR CODE ###\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following cell, test with different `dropout` rates, and observe how that affects the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/3999771966.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y)) # compute loss by calling loss_fn()\n",
      "/var/folders/b6/6366288j429498w43rvsw48w0000gn/T/ipykernel_34598/2174331187.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = loss_fn(pred, torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 11.1%, Avg loss: 2.452429 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.4%, Avg loss: 2.446564 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.1%, Avg loss: 2.447067 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.5%, Avg loss: 2.447204 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 2.442688 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.0%, Avg loss: 2.445226 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.7%, Avg loss: 2.440844 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.5%, Avg loss: 2.451565 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.450969 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 11.9%, Avg loss: 2.448071 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "model2 = NeuralNetModel2(dropout=0.2) # Call NeuralNetModel2() with the dropout value you want to try\n",
    "optimizer = optimizer_adam # You may try Adam/SGD optimizer\n",
    "### END YOUR CODE ###\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    ### START YOUR CODE ###\n",
    "    train_loop(train_loader, model2, loss_fn, optimizer, verbose=False) # Use verbose=False, if you want to see less information\n",
    "    test_loop(test_loader, model2, loss_fn)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Expected output**\n",
    "\n",
    "In theory, you should see that the larger dropout rate you use, the lower test accuracy you will get, at the same epoch number.\n",
    "\n",
    "But the model trained with some dropout rate should generalize better to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
