{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PjpuxXR8t7m"
   },
   "source": [
    "# CS549 Machine Learning\n",
    "# Assignment 12: Transformer and Transformer-based Models (part 2)\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will: \n",
    "1) Implement the **multiple head attention** sub layer in a transformer encoder.\n",
    "\n",
    "2) Play with the transformer-based models provided in **transformers** for multiple natural language processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "07iMXEUK8t7r"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA60pT4u8t7t"
   },
   "source": [
    "## Task 2. Play with transformer-based models\n",
    "**Points: 5**\n",
    "\n",
    "### 2.1 Installation\n",
    "Install the *transformers* package with the following command:\n",
    "```\n",
    "pip install transformers\n",
    "```\n",
    "\n",
    "After it is done, you can load some pretrained BERT models and tokenizers like this (you can ignore the warnings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XHX5F-6t8t7u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (3.12.4)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (1.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/joaquinmendoza/Library/Python/3.11/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/4d/d3/38b09813a32618acd437906c4d0194119e27139dbcd7486e69d58e375a27/regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/c5/0e/8961075de3aca5435fa6371088d44594cdc0e59b5b935afdaf1af028cf36/tokenizers-0.15.0-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/0c/84/1e59b0594ca421ff308169b0802d72adcce359619925141b69d2ebc89269/safetensors-0.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp311-cp311-macosx_11_0_arm64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.0-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.19.4 regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 72.3kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.66MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.32MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 1.68MB/s]\n",
      "model.safetensors: 100%|██████████| 440M/440M [00:11<00:00, 37.7MB/s] \n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvvIhQyK8t7u"
   },
   "source": [
    "### 2.2 Tokenizing inputs\n",
    "\n",
    "Run the following examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AIaKtTur8t7v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "torch.Size([1, 275])\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic\n",
    "resources for human beings. From a thermodynamic point of view, any energy conversion systems\n",
    "that receive energy from the sun and/or dissipate energy to the universe are heat engines with\n",
    "photons as the \"working fluid\" and can be analyzed using the concept of entropy. While entropy\n",
    "analysis provides a particularly convenient way to understand the efficiency limits, it is typically\n",
    "taught in the context of thermodynamic cycles among quasi-equilibrium states and its\n",
    "generalization to solar energy conversion systems running in a continuous and non-equilibrium\n",
    "fashion is not straightforward. In this educational article, we present a few examples to illustrate\n",
    "how the concept of photon entropy, combined with the radiative transfer equation, can be used to\n",
    "analyze the local entropy generation processes and the efficiency limits of different solar energy\n",
    "conversion systems. We provide explicit calculations for the local and total entropy generation\n",
    "rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily\n",
    "reproduced by students. We further discuss the connection between the entropy generation and the\n",
    "device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical\n",
    "efficiency limit.\"\"\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "print(len(text.split()))\n",
    "print(encoded_input['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDCvvBEW8t7w"
   },
   "source": [
    "Can you explain why the `encoded_input` has more elements than the actual number of words in `text`?\\\n",
    "(**Points: 1**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tQjqNrgl8t7x"
   },
   "outputs": [],
   "source": [
    "### Write your answer within the quotes ###\n",
    "answer = \"\"\"\n",
    "    Encoded_input has more elements than the actual number of words in text because of tokenization. \n",
    "    When we tokenize an input text, we store fixed-size chunks of the text which can be larger that the number of words in the text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGFYF1Mg8t7y"
   },
   "source": [
    "*NOTE*: there is no expected output for this question.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Output word vectors from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Erf0c1KW8t7y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 768])\n"
     ]
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "\n",
    "print(last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3L9vmOBu8t7z"
   },
   "source": [
    "With the following code, you can find the corresponding token of each integer id in `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "59jfY6DG8t7z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 2980, 2791, 1997, 1996, 3103, 1998, 1996, 3147]\n",
      "['[CLS]', 'the', 'hot', '##ness', 'of', 'the', 'sun', 'and', 'the', 'cold']\n"
     ]
    }
   ],
   "source": [
    "input_ids_pt = encoded_input['input_ids']\n",
    "input_ids_list = input_ids_pt.tolist()[0]\n",
    "input_tokens = tokenizer.convert_ids_to_tokens(input_ids_list)\n",
    "\n",
    "print(input_ids_list[:10])\n",
    "print(input_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1HiiCNT8t70"
   },
   "source": [
    "Can you find the output vector**s** among `last_hidden_state` that correpond to the input word \"entropy\"?\\\n",
    "Do they have the same values?\\\n",
    "**(Points: 1)**\n",
    "\n",
    "*Hint*: You can use a `if` statement to check if the current token is the word \"entropy\", and if so, you can append it to `vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-AMwmmSr8t70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"entropy\": 6\n",
      "Do they have the same value? [False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "vectors = []\n",
    "for i, token in enumerate(input_tokens):\n",
    "    ### START YOUR CODE ###\n",
    "    if token == \"entropy\":\n",
    "        vectors.append(last_hidden_state[0][i])\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "# Do not change the code below\n",
    "print('Number of \"entropy\":', len(vectors))\n",
    "\n",
    "matches = [torch.allclose(vectors[i], vectors[i+1]) for i in range(len(vectors)-1)]\n",
    "print(f'Do they have the same value? {matches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySpNfE9I8t71"
   },
   "source": [
    "**Expected output:** \\\n",
    "Number of \"entropy\": 6\\\n",
    "Do they have the same value? [False, False, False, False, False]\n",
    "\n",
    "---\n",
    "### 2.4 Sentence vectors from BERT\n",
    "\n",
    "We can obtain the output vectors for a batch of sentences.\n",
    "\n",
    "First, we need to break the text into a list of sentences, using a simple end-of-sentence str '.' as a separater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G9SbxEgO8t71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting in 6 sentences:\n",
      "['The hotness of the sun and the coldness of the outer space are inexhaustible thermodynamic resources for human beings.', 'From a thermodynamic point of view, any energy conversion systems that receive energy from the sun and/or dissipate energy to the universe are heat engines with photons as the \"working fluid\" and can be analyzed using the concept of entropy.', 'While entropy analysis provides a particularly convenient way to understand the efficiency limits, it is typically taught in the context of thermodynamic cycles among quasi-equilibrium states and its generalization to solar energy conversion systems running in a continuous and non-equilibrium fashion is not straightforward.', 'In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.', 'We provide explicit calculations for the local and total entropy generation rates for simple emitters and absorbers, as well as photovoltaic cells, which can be readily reproduced by students.', 'We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.replace('\\n', ' ').split('.')\n",
    "sentences = [s.strip() + '.' for s in sentences if len(s.strip())>0] # Some cleaning work\n",
    "\n",
    "print(f'Resulting in {len(sentences)} sentences:')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBT2qJOw8t71"
   },
   "source": [
    "Now, let's use tokenizer on this batch of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "O3ZqqmVg8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57])\n",
      "tensor([  101,  1996,  2980,  2791,  1997,  1996,  3103,  1998,  1996,  3147,\n",
      "         2791,  1997,  1996,  6058,  2686,  2024,  1999, 10288, 13821,  3775,\n",
      "         3468,  1996, 10867,  7716, 18279,  7712,  4219,  2005,  2529,  9552,\n",
      "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = tokenizer(sentences, padding=True, return_tensors='pt')\n",
    "\n",
    "print(encoded_sentences['input_ids'].shape)\n",
    "print(encoded_sentences['input_ids'][0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYA7h9OO8t72"
   },
   "source": [
    "You can find that shorter sentences are padded with a special id `0`.\n",
    "\n",
    "Next, we can obtain the output tensors for all input sentences, also in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ti6zE5Ng8t72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 57, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**encoded_sentences)\n",
    "\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcLgx9xI8t72"
   },
   "source": [
    "Note that the first dimension of `outputs['last_hidden_state']` is batch size. So the output tensor for the 1st sentence is `outputs['last_hidden_state'][0]`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kV2WoMCU8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([57, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs['last_hidden_state'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3e9KZx28t73"
   },
   "source": [
    "For each output tensor, the first 768-dim vector (at position 0) always corresponds to the special input token `[CLS]`. We can use this vector to represent the meaning of the whole sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cim3ChPi8t73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "CLS_vec = outputs['last_hidden_state'][0][0]\n",
    "print(CLS_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWopMVKR8t74"
   },
   "source": [
    "Now, it is your task to compute the cosine similarities between each pair of the 6 sentences, and find the pair that has the closest meanings.\\\n",
    "**(Points: 3)**\n",
    "\n",
    "*Hint*: You can use the `cosine_similarity()` function imported at the beginning, which takes input two tensors and returns the similarity score in a tensor. So you will need to append a `.item()` to retrieve the numeric value from the returned tensor. You also need to specify the argument `dim=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "v5OQcKmD8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <-> 1: -1.0\n",
      "0 <-> 2: 1.0\n",
      "0 <-> 3: -1.0\n",
      "0 <-> 4: 1.0\n",
      "0 <-> 5: 1.0\n",
      "1 <-> 2: -1.0\n",
      "1 <-> 3: 1.0\n",
      "1 <-> 4: -1.0\n",
      "1 <-> 5: -1.0\n",
      "2 <-> 3: -1.0\n",
      "2 <-> 4: 1.0\n",
      "2 <-> 5: 1.0\n",
      "3 <-> 4: -1.0\n",
      "3 <-> 5: -1.0\n",
      "4 <-> 5: 1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for j in range(i+1, 6):\n",
    "        ### START YOUR CODE ###\n",
    "        vec_i = CLS_vec[i]\n",
    "        vec_j = CLS_vec[j]\n",
    "        sim = cosine_similarity(vec_i, vec_j, dim=0).item() # Hint: when you call cosine_similarity() function, remember to specify dim=0. Also, you need append .item() at the end to obtain a number instead of a tensor.\n",
    "        ### END YOUR CODE ###\n",
    "        print(f'{i} <-> {j}: {sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gf5xshFV8t74"
   },
   "source": [
    "**Expected output:**\\\n",
    "0 <-> 1: 0.8591639399528503\\\n",
    "0 <-> 2: 0.777198314666748\\\n",
    "0 <-> 3: 0.7985224723815918\\\n",
    "0 <-> 4: 0.7754684090614319\\\n",
    "0 <-> 5: 0.8052163124084473\\\n",
    "1 <-> 2: 0.876341700553894\\\n",
    "1 <-> 3: 0.8321619629859924\\\n",
    "1 <-> 4: 0.823844850063324\\\n",
    "1 <-> 5: 0.8492751717567444\\\n",
    "2 <-> 3: 0.8241377472877502\\\n",
    "2 <-> 4: 0.8598626852035522\\\n",
    "2 <-> 5: 0.8579834699630737\\\n",
    "3 <-> 4: 0.9018082618713379\\\n",
    "3 <-> 5: 0.929144024848938\\\n",
    "4 <-> 5: 0.9185266494750977\n",
    "\n",
    "---\n",
    "You can print out the two sentences to see if the similarity score makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1BhZGMKc8t74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this educational article, we present a few examples to illustrate how the concept of photon entropy, combined with the radiative transfer equation, can be used to analyze the local entropy generation processes and the efficiency limits of different solar energy conversion systems.\n",
      "We further discuss the connection between the entropy generation and the device efficiency, particularly the exact spectral matching condition that is shared by infinitejunction photovoltaic cells and reversible thermoelectric materials to approach their theoretical efficiency limit.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "print(sentences[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngfL2vK38t75"
   },
   "source": [
    "---\n",
    "\n",
    "### 2.5 Play with summarization\n",
    "\n",
    "Lastly, let's play with the summarization pipelien provided by transformers. Be patient when the model is downloading. \n",
    "\n",
    "You can try the following code with different input text or arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ro8Ay1688t75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "config.json: 100%|██████████| 1.80k/1.80k [00:00<00:00, 7.04MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.22G/1.22G [00:34<00:00, 35.2MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 125kB/s]\n",
      "vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.37MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': ' The hotness of the sun and the coldness of outer space are inexhaustible thermodynamic resources for human beings . From a thermodynamic point of view, any energy conversion systems that receive energy from the sun or dissipate energy to the universe are heat engines with photons as the \"working fluid\"'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "print(summarizer(text, max_length=150, min_length=30))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
